{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alireza8Kh/the-DDPG-Method-Implemented-in-Pendulum-v1-Environment/blob/main/Copy_of_Pendulum_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G3elMwDhOkOz"
      },
      "outputs": [],
      "source": [
        "#!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "20IyxDzgp3tU"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "\n",
        "import os\n",
        "import gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Graphical library\n",
        "import random\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from scipy.stats import ttest_1samp\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Configuring Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Tox87FUXwD"
      },
      "source": [
        "**Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GmBYBmriPSZ",
        "outputId": "3be37ab0-e730-40fc-8493-36c81791ac66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment PendulumPrediction-v1\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Pendulum-v1', g = 9.81)\n",
        "\n",
        "from gym.wrappers.time_limit import TimeLimit\n",
        "\n",
        "\n",
        "class ShowPendulumPredictions(gym.Env):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.env = gym.make('Pendulum-v1', g=9.81)\n",
        "        self.action_space = self.env.action_space\n",
        "        self.observation_space = self.env.observation_space\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "\n",
        "        return self.env.render(mode=mode)\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Close the environment.\n",
        "        \"\"\"\n",
        "        self.env.close()\n",
        "\n",
        "gym.envs.registration.register( id=\"PendulumPrediction-v1\", entry_point = ShowPendulumPredictions, max_episode_steps=999 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKHIOPm_UwK4"
      },
      "source": [
        "**the MLP class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZrrOYjZfcEIA"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model):\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.Linear):  # Check if the layer is a linear layer\n",
        "            nn.init.xavier_uniform_(layer.weight)  # Xavier initialization for weights\n",
        "            if layer.bias is not None:  # Initialize biases to zeros\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, num_hidden, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialise the network.\n",
        "        Input:\n",
        "            - input_size {int} - size of input to the network\n",
        "            - output_size {int} - size of output to the network\n",
        "            - num_hidden {int} - number of hidden layers\n",
        "            - hidden_size {int} - size of each hidden layer\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size) # First tranformation from the network input to the input of first hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden-1)]) # All the hidden transformation\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size) # Last tranformation from the last hidden layer output to the network output\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Get the output of the MLP.\n",
        "        Input: x {tensor} - one element or a batch of element\n",
        "        Ouput: y {tensor} - corresponding output\n",
        "        \"\"\"\n",
        "        x.to(device)\n",
        "        x = self.input_layer(x) # Passing through the input layer\n",
        "        x = F.relu(x) # Applying Relu activation\n",
        "        for layer in self.hidden_layers:\n",
        "          x = layer(x) # Passing through each hidden layer\n",
        "          x = F.relu(x) # Applying Relu activation\n",
        "        x = self.output_layer(x) # Passing through the output layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lBcIUZ1x6Bg"
      },
      "source": [
        "**Batching the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aydDmDSz3olA"
      },
      "outputs": [],
      "source": [
        "def batch_data(state_list, action_list, reward_list, next_state_list, batch_size):\n",
        "    num_samples = len(state_list)\n",
        "    sample_indices = random.sample(range(num_samples), batch_size)\n",
        "\n",
        "\n",
        "    # Ensure tensors are correctly reshaped\n",
        "    batched_state = torch.cat(\n",
        "        [torch.tensor(state_list[i], dtype=torch.float32).view(1, -1) for i in sample_indices], dim=0\n",
        "    )\n",
        "    batched_action = torch.cat(\n",
        "        [torch.tensor(action_list[i], dtype=torch.float32).view(1, -1) for i in sample_indices], dim=0\n",
        "    )\n",
        "    batched_reward = torch.cat(\n",
        "        [torch.tensor([reward_list[i]], dtype=torch.float32).view(1, -1) for i in sample_indices], dim=0\n",
        "    )\n",
        "    batched_next_state = torch.cat(\n",
        "        [torch.tensor(next_state_list[i], dtype=torch.float32).view(1, -1) for i in sample_indices], dim=0\n",
        "    )\n",
        "\n",
        "    return batched_state, batched_action, batched_reward, batched_next_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI9jgtoRdE0B"
      },
      "source": [
        "**Some Usefull Modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Jek9Y9dvjLvr"
      },
      "outputs": [],
      "source": [
        "def MSE_loss(prediction, target):\n",
        "\n",
        "      return ((prediction - target)**2).sum(dim=-1).mean()\n",
        "\n",
        "def soft_update(target_network, predictor_network, tau):\n",
        "        \"\"\"\n",
        "        Softly updates the parameters of the target network.\n",
        "        target_param = tau * predictor_param + (1 - tau) * target_param\n",
        "        \"\"\"\n",
        "        for target_param, predictor_param in zip(target_network.parameters(), predictor_network.parameters()):\n",
        "            target_param.data.copy_(tau * predictor_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "def reset(self):\n",
        "        \"\"\"Reset the noise process to the mean.\"\"\"\n",
        "        self.x_prev = np.zeros_like(self.mu)\n",
        "\n",
        "def sample(self):\n",
        "        \"\"\"Generate the next noise value.\"\"\"\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mu - self.x_prev) * self.dt\n",
        "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.action_dim)\n",
        "        )\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "def soft_update(target, source, tau):\n",
        "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
        "            target_param.data.copy_(tau * source_param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "def moving_average(data, window_size=10):\n",
        "      return [sum(data[i:i+window_size])/window_size for i in range(len(data) - window_size)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivR2_ZwNOneW"
      },
      "source": [
        "**Training Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEg5wUIZoZaD",
        "outputId": "9796498e-0d34-4386-e9c5-7ba3f43f18f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Total Reward: -1627.21537363, Actor Loss: 7.9652109146118164, Last Twenty Mean: 8.3166002511978157\n",
            "Epoch 3, Total Reward: -1588.36133878, Actor Loss: 12.1045360565185547, Last Twenty Mean: 11.7914289474487308\n",
            "Epoch 4, Total Reward: -1211.74382658, Actor Loss: 14.9844255447387695, Last Twenty Mean: 15.3070602893829353\n",
            "Epoch 5, Total Reward: -1550.00659779, Actor Loss: 21.3035106658935547, Last Twenty Mean: 21.1499678611755364\n",
            "Epoch 6, Total Reward: -1154.68472616, Actor Loss: 25.8284797668457031, Last Twenty Mean: 25.9577215194702156\n",
            "Epoch 7, Total Reward: -1643.58062031, Actor Loss: 32.4551544189453125, Last Twenty Mean: 32.7162058830261202\n",
            "Epoch 8, Total Reward: -1190.25223422, Actor Loss: 37.3670043945312500, Last Twenty Mean: 38.2547292709350586\n",
            "Epoch 9, Total Reward: -1495.90850700, Actor Loss: 44.4995346069335938, Last Twenty Mean: 44.4633220672607408\n",
            "Epoch 10, Total Reward: -1209.12563634, Actor Loss: 49.9528274536132812, Last Twenty Mean: 50.0120773315429688\n",
            "Epoch 11, Total Reward: -1234.35083827, Actor Loss: 54.1974906921386719, Last Twenty Mean: 54.6482011795043974\n",
            "Epoch 12, Total Reward: -759.88236228, Actor Loss: 55.5080261230468750, Last Twenty Mean: 58.1561529159545927\n",
            "Epoch 13, Total Reward: -872.47485159, Actor Loss: 61.6588287353515625, Last Twenty Mean: 61.2174364089965835\n",
            "Epoch 14, Total Reward: -1100.38910089, Actor Loss: 66.2239837646484375, Last Twenty Mean: 64.8232200622558565\n",
            "Epoch 15, Total Reward: -1219.33699096, Actor Loss: 67.4669799804687500, Last Twenty Mean: 67.1685066223144531\n",
            "Epoch 16, Total Reward: -795.31845820, Actor Loss: 70.4826660156250000, Last Twenty Mean: 70.8777984619140682\n",
            "Epoch 17, Total Reward: -1058.78890483, Actor Loss: 74.1461563110351562, Last Twenty Mean: 72.6997791290283146\n",
            "Epoch 18, Total Reward: -513.59314048, Actor Loss: 75.3607025146484375, Last Twenty Mean: 74.4138183593750000\n",
            "Epoch 19, Total Reward: -429.41251243, Actor Loss: 74.5611877441406250, Last Twenty Mean: 74.8744937896728544\n",
            "Epoch 20, Total Reward: -417.50684439, Actor Loss: 70.3794021606445312, Last Twenty Mean: 74.8605144500732393\n",
            "Epoch 21, Total Reward: -6.84775288, Actor Loss: 73.9027557373046875, Last Twenty Mean: 76.1400466918945256\n",
            "Epoch 22, Total Reward: -376.30029971, Actor Loss: 77.3994216918945312, Last Twenty Mean: 72.5788181304931612\n",
            "Epoch 23, Total Reward: -249.13871192, Actor Loss: 71.8719100952148438, Last Twenty Mean: 72.9410594940185604\n",
            "Epoch 24, Total Reward: -126.77923844, Actor Loss: 71.4260711669921875, Last Twenty Mean: 70.1267524719238224\n",
            "Epoch 25, Total Reward: -129.71121990, Actor Loss: 69.0961608886718750, Last Twenty Mean: 70.4015211105346737\n",
            "Epoch 26, Total Reward: -366.32423973, Actor Loss: 72.5954742431640625, Last Twenty Mean: 69.2814147949218722\n",
            "Epoch 27, Total Reward: -128.92257189, Actor Loss: 67.6902465820312500, Last Twenty Mean: 68.4797643661499080\n",
            "Epoch 28, Total Reward: -135.58897943, Actor Loss: 63.6840972900390625, Last Twenty Mean: 65.9095737457275419\n",
            "Epoch 29, Total Reward: -136.89897871, Actor Loss: 63.7926330566406250, Last Twenty Mean: 63.0801601409912109\n",
            "Epoch 30, Total Reward: -144.35006521, Actor Loss: 52.8004455566406250, Last Twenty Mean: 61.9633388519287109\n",
            "Epoch 31, Total Reward: -389.84969352, Actor Loss: 63.1898422241210938, Last Twenty Mean: 60.2488265991210952\n",
            "Epoch 32, Total Reward: -623.26066640, Actor Loss: 56.0015411376953125, Last Twenty Mean: 58.3857677459716768\n",
            "Epoch 33, Total Reward: -642.46297381, Actor Loss: 58.2954025268554688, Last Twenty Mean: 59.0256011962890597\n",
            "Epoch 34, Total Reward: -634.37310578, Actor Loss: 60.7598342895507812, Last Twenty Mean: 59.4730665206909208\n",
            "Epoch 35, Total Reward: -520.81448897, Actor Loss: 54.5745773315429688, Last Twenty Mean: 58.9964651107788072\n",
            "Epoch 36, Total Reward: -133.98743398, Actor Loss: 54.7335472106933594, Last Twenty Mean: 55.2474029541015597\n",
            "Epoch 37, Total Reward: -134.46602896, Actor Loss: 56.5959930419921875, Last Twenty Mean: 56.8633417129516587\n",
            "Epoch 38, Total Reward: -133.48161400, Actor Loss: 57.9513397216796875, Last Twenty Mean: 54.5057098388671903\n",
            "Epoch 39, Total Reward: -257.43243339, Actor Loss: 44.0896453857421875, Last Twenty Mean: 52.3183137893676786\n",
            "Epoch 40, Total Reward: -131.30374442, Actor Loss: 43.3475570678710938, Last Twenty Mean: 50.3504272460937514\n",
            "Epoch 41, Total Reward: -128.59954396, Actor Loss: 49.0915374755859375, Last Twenty Mean: 49.6642433166503920\n",
            "Epoch 42, Total Reward: -116.47842655, Actor Loss: 46.2057952880859375, Last Twenty Mean: 48.8122261047363253\n",
            "Epoch 43, Total Reward: -126.76798427, Actor Loss: 41.1282768249511719, Last Twenty Mean: 46.8721233367919936\n",
            "Epoch 44, Total Reward: -129.84607845, Actor Loss: 47.9832420349121094, Last Twenty Mean: 45.4679746627807617\n",
            "Epoch 45, Total Reward: -120.84448754, Actor Loss: 46.3003616333007812, Last Twenty Mean: 45.6813819885253878\n",
            "Epoch 46, Total Reward: -131.41126515, Actor Loss: 42.1393547058105469, Last Twenty Mean: 42.7120351791381836\n",
            "Epoch 47, Total Reward: -506.39621097, Actor Loss: 49.2334136962890625, Last Twenty Mean: 41.7777336120605440\n",
            "Epoch 48, Total Reward: -623.66042949, Actor Loss: 37.4345703125000000, Last Twenty Mean: 41.5074466705322251\n",
            "Epoch 49, Total Reward: -395.54607942, Actor Loss: 48.5222434997558594, Last Twenty Mean: 42.7066328048706083\n",
            "Epoch 50, Total Reward: -636.72949004, Actor Loss: 42.2599143981933594, Last Twenty Mean: 42.8059700012207003\n",
            "Epoch 51, Total Reward: -524.43867545, Actor Loss: 40.2324523925781250, Last Twenty Mean: 40.5176932334899931\n",
            "Epoch 52, Total Reward: -504.65370681, Actor Loss: 37.0510559082031250, Last Twenty Mean: 41.4965074539184542\n",
            "Epoch 53, Total Reward: -514.17520166, Actor Loss: 38.7979774475097656, Last Twenty Mean: 40.1989386558532686\n",
            "Epoch 54, Total Reward: -397.87940890, Actor Loss: 39.9294929504394531, Last Twenty Mean: 39.8383122444152846\n",
            "Epoch 55, Total Reward: -498.74443971, Actor Loss: 43.7491188049316406, Last Twenty Mean: 39.5025875091552763\n",
            "Epoch 56, Total Reward: -519.54850531, Actor Loss: 45.7947769165039062, Last Twenty Mean: 38.3092378616332994\n",
            "Epoch 57, Total Reward: -268.33159356, Actor Loss: 39.8351211547851562, Last Twenty Mean: 38.0325496673583956\n",
            "Epoch 58, Total Reward: -132.74237380, Actor Loss: 34.9090614318847656, Last Twenty Mean: 37.4288188934326200\n",
            "Epoch 59, Total Reward: -254.14473661, Actor Loss: 38.0467872619628906, Last Twenty Mean: 35.5890152931213350\n",
            "Epoch 60, Total Reward: -391.25591445, Actor Loss: 28.0216102600097656, Last Twenty Mean: 35.1189873695373507\n",
            "Epoch 61, Total Reward: -637.99983202, Actor Loss: 35.5840911865234375, Last Twenty Mean: 36.4611634254455552\n",
            "Epoch 62, Total Reward: -637.54584453, Actor Loss: 37.4839324951171875, Last Twenty Mean: 35.8628886222839327\n",
            "Epoch 63, Total Reward: -524.84286503, Actor Loss: 33.8057250976562500, Last Twenty Mean: 34.1385512351989746\n",
            "Epoch 64, Total Reward: -270.13177521, Actor Loss: 35.8727645874023438, Last Twenty Mean: 35.4869333267211928\n",
            "Epoch 65, Total Reward: -329.74858579, Actor Loss: 33.1717414855957031, Last Twenty Mean: 33.4471850395202637\n",
            "Epoch 66, Total Reward: -145.75795982, Actor Loss: 34.6467857360839844, Last Twenty Mean: 36.3112638473510714\n",
            "Epoch 67, Total Reward: -270.93562294, Actor Loss: 38.8128509521484375, Last Twenty Mean: 34.4825769424438491\n",
            "Epoch 68, Total Reward: -134.60764633, Actor Loss: 31.5562362670898438, Last Twenty Mean: 34.1224220275878878\n",
            "Epoch 69, Total Reward: -131.49807713, Actor Loss: 33.6031379699707031, Last Twenty Mean: 30.4220767974853530\n",
            "Epoch 70, Total Reward: -124.30505976, Actor Loss: 37.5517005920410156, Last Twenty Mean: 32.0249134063720717\n",
            "Epoch 71, Total Reward: -117.63818661, Actor Loss: 36.0586433410644531, Last Twenty Mean: 31.1404780387878404\n",
            "Epoch 72, Total Reward: -124.07711000, Actor Loss: 31.5555934906005859, Last Twenty Mean: 30.5185132980346694\n",
            "Epoch 73, Total Reward: -2.09228924, Actor Loss: 32.6811828613281250, Last Twenty Mean: 30.1101100921630866\n",
            "Epoch 74, Total Reward: -242.63878713, Actor Loss: 27.9058494567871094, Last Twenty Mean: 28.9826853752136238\n",
            "Epoch 75, Total Reward: -125.85831611, Actor Loss: 26.8779144287109375, Last Twenty Mean: 27.6262697219848619\n",
            "Epoch 76, Total Reward: -0.09961672, Actor Loss: 32.9264869689941406, Last Twenty Mean: 27.2686027526855455\n",
            "Epoch 77, Total Reward: -0.15584119, Actor Loss: 27.8109416961669922, Last Twenty Mean: 27.0741680145263679\n",
            "Epoch 78, Total Reward: -125.05100369, Actor Loss: 26.6097393035888672, Last Twenty Mean: 26.9653594970703132\n",
            "Epoch 79, Total Reward: -248.97468487, Actor Loss: 20.8160495758056641, Last Twenty Mean: 26.0189877510070815\n",
            "Epoch 80, Total Reward: -138.33707912, Actor Loss: 30.4412689208984375, Last Twenty Mean: 27.2479351043701179\n",
            "Epoch 81, Total Reward: -245.23383315, Actor Loss: 24.2387466430664062, Last Twenty Mean: 25.6766049385070794\n",
            "Epoch 82, Total Reward: -128.58990051, Actor Loss: 22.5333309173583984, Last Twenty Mean: 26.2738722801208482\n",
            "Epoch 83, Total Reward: -251.42340125, Actor Loss: 21.3426380157470703, Last Twenty Mean: 26.0521038055419929\n",
            "Epoch 84, Total Reward: -123.05325961, Actor Loss: 23.1268920898437500, Last Twenty Mean: 25.7402585029602058\n",
            "Epoch 85, Total Reward: -1.99795779, Actor Loss: 18.4702663421630859, Last Twenty Mean: 24.3671982288360596\n",
            "Epoch 86, Total Reward: -130.89656086, Actor Loss: 24.9811916351318359, Last Twenty Mean: 25.0924270629882820\n",
            "Epoch 87, Total Reward: -241.37615613, Actor Loss: 28.4950046539306641, Last Twenty Mean: 23.9849862575530999\n",
            "Epoch 88, Total Reward: -9.82200490, Actor Loss: 23.8771839141845703, Last Twenty Mean: 23.1408105850219741\n",
            "Epoch 89, Total Reward: -131.53374071, Actor Loss: 17.0239391326904297, Last Twenty Mean: 22.9347781181335435\n",
            "Epoch 90, Total Reward: -141.84158930, Actor Loss: 22.2819919586181641, Last Twenty Mean: 23.3977616310119636\n",
            "Epoch 91, Total Reward: -140.49722460, Actor Loss: 24.5900344848632812, Last Twenty Mean: 21.8718844413757338\n",
            "Epoch 92, Total Reward: -136.58276359, Actor Loss: 25.4642696380615234, Last Twenty Mean: 22.1403265476226814\n",
            "Epoch 93, Total Reward: -19.19271064, Actor Loss: 20.9203300476074219, Last Twenty Mean: 22.1630363941192634\n",
            "Epoch 94, Total Reward: -245.11967854, Actor Loss: 16.6483688354492188, Last Twenty Mean: 21.9914785385131850\n",
            "Epoch 95, Total Reward: -310.74310112, Actor Loss: 22.9471740722656250, Last Twenty Mean: 21.8294902801513686\n",
            "Epoch 96, Total Reward: -144.41560979, Actor Loss: 20.7730846405029297, Last Twenty Mean: 21.6474141120910630\n",
            "Epoch 97, Total Reward: -276.96371523, Actor Loss: 27.5143890380859375, Last Twenty Mean: 20.8428181648254380\n",
            "Epoch 98, Total Reward: -269.66608783, Actor Loss: 13.6818504333496094, Last Twenty Mean: 20.8506315708160415\n",
            "Epoch 99, Total Reward: -270.58401872, Actor Loss: 16.0386619567871094, Last Twenty Mean: 19.7570637226104751\n",
            "Epoch 100, Total Reward: -276.94508185, Actor Loss: 21.6689128875732422, Last Twenty Mean: 21.2635525703430162\n",
            "Epoch 101, Total Reward: -144.80834535, Actor Loss: 23.4393634796142578, Last Twenty Mean: 19.6737547397613532\n",
            "Epoch 102, Total Reward: -259.06040588, Actor Loss: 20.4297676086425781, Last Twenty Mean: 19.3619609832763686\n",
            "Epoch 103, Total Reward: -138.65045699, Actor Loss: 25.1032161712646484, Last Twenty Mean: 18.2695572376251221\n",
            "Epoch 104, Total Reward: -139.02147021, Actor Loss: 18.2790069580078125, Last Twenty Mean: 18.8444814682006836\n",
            "Epoch 105, Total Reward: -263.33048529, Actor Loss: 20.1991043090820312, Last Twenty Mean: 19.4627561569213867\n",
            "Epoch 106, Total Reward: -270.12162785, Actor Loss: 18.0116786956787109, Last Twenty Mean: 19.1891817569732659\n",
            "Epoch 107, Total Reward: -150.89939266, Actor Loss: 16.7654266357421875, Last Twenty Mean: 18.4584084510803237\n",
            "Epoch 108, Total Reward: -270.84081692, Actor Loss: 12.9740190505981445, Last Twenty Mean: 18.0790730953216539\n",
            "Epoch 109, Total Reward: -270.06858151, Actor Loss: 18.7067794799804688, Last Twenty Mean: 18.2606173992156968\n",
            "Epoch 110, Total Reward: -486.70201204, Actor Loss: 16.9435234069824219, Last Twenty Mean: 18.2165120124816902\n",
            "Epoch 111, Total Reward: -509.15998727, Actor Loss: 22.3941307067871094, Last Twenty Mean: 18.6234712600708008\n",
            "Epoch 112, Total Reward: -514.41517789, Actor Loss: 16.3356838226318359, Last Twenty Mean: 17.4582026004791260\n",
            "Epoch 113, Total Reward: -253.21442760, Actor Loss: 16.2437000274658203, Last Twenty Mean: 16.7722344875335700\n",
            "Epoch 114, Total Reward: -385.85992598, Actor Loss: 13.4071207046508789, Last Twenty Mean: 16.2307597637176499\n",
            "Epoch 115, Total Reward: -228.00009043, Actor Loss: 22.8106632232666016, Last Twenty Mean: 16.9465174198150628\n",
            "Epoch 116, Total Reward: -133.51417309, Actor Loss: 20.7402801513671875, Last Twenty Mean: 17.4322016716003425\n",
            "Epoch 117, Total Reward: -141.45249836, Actor Loss: 10.9491949081420898, Last Twenty Mean: 15.5668112277984623\n",
            "Epoch 118, Total Reward: -282.09648256, Actor Loss: 17.2097339630126953, Last Twenty Mean: 17.9315324306488044\n",
            "Epoch 119, Total Reward: -255.96952201, Actor Loss: 18.7549953460693359, Last Twenty Mean: 17.1242024421691887\n",
            "Epoch 120, Total Reward: -10.17112555, Actor Loss: 9.5533065795898438, Last Twenty Mean: 16.2051351547241218\n",
            "Epoch 121, Total Reward: -133.25311972, Actor Loss: 17.8271293640136719, Last Twenty Mean: 15.1882122516632077\n",
            "Epoch 122, Total Reward: -134.03537005, Actor Loss: 13.8986997604370117, Last Twenty Mean: 13.4396013259887699\n",
            "Epoch 123, Total Reward: -10.43523051, Actor Loss: 15.2523126602172852, Last Twenty Mean: 13.7159735202789310\n",
            "Epoch 124, Total Reward: -368.83209662, Actor Loss: 12.0723094940185547, Last Twenty Mean: 14.1754426002502445\n",
            "Epoch 125, Total Reward: -263.71153747, Actor Loss: 7.6984386444091797, Last Twenty Mean: 13.2103532075881951\n",
            "Epoch 126, Total Reward: -13.63316612, Actor Loss: 12.5839872360229492, Last Twenty Mean: 13.2481876373291012\n",
            "Epoch 127, Total Reward: -145.79558665, Actor Loss: 14.5044116973876953, Last Twenty Mean: 14.8868405342102044\n",
            "Epoch 128, Total Reward: -141.46249013, Actor Loss: 13.6227684020996094, Last Twenty Mean: 12.9436099052429192\n",
            "Epoch 129, Total Reward: -6.04473981, Actor Loss: 7.9195623397827148, Last Twenty Mean: 12.5368617057800300\n",
            "Epoch 130, Total Reward: -138.06911604, Actor Loss: 12.7964534759521484, Last Twenty Mean: 10.6242075681686394\n",
            "Epoch 131, Total Reward: -265.29147627, Actor Loss: 15.0559883117675781, Last Twenty Mean: 11.8264695405960083\n",
            "Epoch 132, Total Reward: -145.79504388, Actor Loss: 12.4299659729003906, Last Twenty Mean: 10.6676086664199836\n",
            "Epoch 133, Total Reward: -261.93629680, Actor Loss: 6.4728388786315918, Last Twenty Mean: 11.3134722948074344\n",
            "Epoch 134, Total Reward: -284.79117655, Actor Loss: 9.4415445327758789, Last Twenty Mean: 10.2939288854599003\n",
            "Epoch 135, Total Reward: -517.41004378, Actor Loss: 11.4079170227050781, Last Twenty Mean: 10.9886861801147457\n",
            "Epoch 136, Total Reward: -529.02739375, Actor Loss: 8.6749258041381836, Last Twenty Mean: 11.2631222724914544\n",
            "Epoch 137, Total Reward: -512.10389240, Actor Loss: 11.4598703384399414, Last Twenty Mean: 10.7926943063735958\n",
            "Epoch 138, Total Reward: -521.15080549, Actor Loss: 15.5733766555786133, Last Twenty Mean: 10.0792354583740238\n",
            "Epoch 139, Total Reward: -620.78912210, Actor Loss: 3.0578494071960449, Last Twenty Mean: 9.6208223342895511\n",
            "Epoch 140, Total Reward: -643.72334290, Actor Loss: 11.2107334136962891, Last Twenty Mean: 8.8344977617263787\n",
            "Epoch 141, Total Reward: -645.42399838, Actor Loss: 16.1587486267089844, Last Twenty Mean: 9.4486599266529083\n",
            "Epoch 142, Total Reward: -765.31821632, Actor Loss: 8.8093309402465820, Last Twenty Mean: 8.6897850990295407\n",
            "Epoch 143, Total Reward: -651.33183145, Actor Loss: 14.2239179611206055, Last Twenty Mean: 9.5419422268867500\n",
            "Epoch 144, Total Reward: -736.93300883, Actor Loss: 11.9127540588378906, Last Twenty Mean: 9.4193722724914544\n",
            "Epoch 145, Total Reward: -646.69046567, Actor Loss: 8.4587192535400391, Last Twenty Mean: 8.9633500099182122\n",
            "Epoch 146, Total Reward: -497.40943525, Actor Loss: 10.2421321868896484, Last Twenty Mean: 8.1446463942527778\n",
            "Epoch 147, Total Reward: -134.70228373, Actor Loss: 10.1616115570068359, Last Twenty Mean: 9.5475959777832031\n",
            "Epoch 148, Total Reward: -137.03179975, Actor Loss: 5.2568268775939941, Last Twenty Mean: 8.4202033400535576\n",
            "Epoch 149, Total Reward: -126.17281969, Actor Loss: 9.7366380691528320, Last Twenty Mean: 8.0953046321868900\n",
            "Epoch 150, Total Reward: -268.03164558, Actor Loss: 12.4917545318603516, Last Twenty Mean: 9.9348263263702385\n",
            "Epoch 151, Total Reward: -260.22556301, Actor Loss: 9.8397474288940430, Last Twenty Mean: 9.1048010110855095\n",
            "Epoch 152, Total Reward: -371.20303774, Actor Loss: 6.4793519973754883, Last Twenty Mean: 9.0060471534729007\n",
            "Epoch 153, Total Reward: -155.03624812, Actor Loss: 5.7778921127319336, Last Twenty Mean: 8.8588259458541874\n",
            "Epoch 154, Total Reward: -519.63387786, Actor Loss: 9.7992496490478516, Last Twenty Mean: 8.3072760105133057\n",
            "Epoch 155, Total Reward: -643.14477011, Actor Loss: 8.2227878570556641, Last Twenty Mean: 7.6923831343650821\n",
            "Epoch 156, Total Reward: -514.99156582, Actor Loss: 9.3039321899414062, Last Twenty Mean: 9.0001486301422116\n",
            "Epoch 157, Total Reward: -424.30272606, Actor Loss: 8.8731813430786133, Last Twenty Mean: 9.5881284475326538\n",
            "Epoch 158, Total Reward: -129.93515709, Actor Loss: 5.4542531967163086, Last Twenty Mean: 8.8827986717224121\n",
            "Epoch 159, Total Reward: -293.53980254, Actor Loss: 10.5259237289428711, Last Twenty Mean: 9.5170653223991390\n",
            "Epoch 160, Total Reward: -1482.82187472, Actor Loss: 14.0947589874267578, Last Twenty Mean: 8.2419303297996525\n",
            "Epoch 161, Total Reward: -130.99730954, Actor Loss: 8.8413476943969727, Last Twenty Mean: 7.5633283153176309\n",
            "Epoch 162, Total Reward: -137.66423452, Actor Loss: 5.5882701873779297, Last Twenty Mean: 8.5582476973533623\n",
            "Epoch 163, Total Reward: -129.54367879, Actor Loss: 8.9483346939086914, Last Twenty Mean: 8.8726436138153080\n",
            "Epoch 164, Total Reward: -272.26923661, Actor Loss: 6.1535120010375977, Last Twenty Mean: 6.0782950967550278\n",
            "Epoch 165, Total Reward: -403.71159457, Actor Loss: 9.2503404617309570, Last Twenty Mean: 7.0582796126604084\n",
            "Epoch 166, Total Reward: -132.80324561, Actor Loss: 6.0763711929321289, Last Twenty Mean: 7.4219399333000187\n",
            "Epoch 167, Total Reward: -128.55461902, Actor Loss: 8.0469083786010742, Last Twenty Mean: 7.3918113470077511\n",
            "Epoch 168, Total Reward: -128.63412590, Actor Loss: 8.3460025787353516, Last Twenty Mean: 7.5393044710159298\n",
            "Epoch 169, Total Reward: -343.04569698, Actor Loss: 6.7105693817138672, Last Twenty Mean: 6.6824746966362003\n",
            "Epoch 170, Total Reward: -129.05884899, Actor Loss: 7.5389761924743652, Last Twenty Mean: 6.5983883500099179\n",
            "Epoch 171, Total Reward: -124.63724982, Actor Loss: 9.5613451004028320, Last Twenty Mean: 5.8086745038628580\n",
            "Epoch 172, Total Reward: -141.43688323, Actor Loss: 9.2712497711181641, Last Twenty Mean: 5.4475168704986574\n",
            "Epoch 173, Total Reward: -132.77227439, Actor Loss: 7.2335190773010254, Last Twenty Mean: 4.9603133678436278\n",
            "Epoch 174, Total Reward: -134.27105874, Actor Loss: 6.0825920104980469, Last Twenty Mean: 4.1284535832703115\n",
            "Epoch 175, Total Reward: -134.16320475, Actor Loss: 0.2026587128639221, Last Twenty Mean: 4.0426696628332142\n",
            "Epoch 176, Total Reward: -248.96784540, Actor Loss: 2.1180288791656494, Last Twenty Mean: 3.4858921647071837\n",
            "Epoch 177, Total Reward: -270.56572382, Actor Loss: 3.1375563144683838, Last Twenty Mean: 3.4911970347166061\n",
            "Epoch 178, Total Reward: -259.53825582, Actor Loss: 2.5466742515563965, Last Twenty Mean: 3.3798071652650834\n",
            "Epoch 179, Total Reward: -138.66861658, Actor Loss: 0.0091199874877930, Last Twenty Mean: 2.5871480703353882\n",
            "Epoch 180, Total Reward: -138.51688029, Actor Loss: 2.7483725547790527, Last Twenty Mean: 3.4502401724457741\n",
            "Epoch 181, Total Reward: -420.42582686, Actor Loss: -2.3175663948059082, Last Twenty Mean: 2.6389350116252901\n",
            "Epoch 182, Total Reward: -279.39176090, Actor Loss: 5.9596042633056641, Last Twenty Mean: 2.9278501570224762\n",
            "Epoch 183, Total Reward: -1495.11546745, Actor Loss: 2.2183146476745605, Last Twenty Mean: 4.1853936731815340\n"
          ]
        }
      ],
      "source": [
        "state_size = 3  # [cos(theta), sin(theta), theta_dot]\n",
        "action_size = 1  # Torque\n",
        "num_hidden = 2\n",
        "\n",
        "# Define networks\n",
        "predictor_actor = MLP(state_size, action_size, num_hidden, hidden_size=512)\n",
        "predictor_critic = MLP(state_size + action_size, 1, num_hidden, hidden_size=512)\n",
        "initialize_weights(predictor_actor)\n",
        "initialize_weights(predictor_critic)\n",
        "\n",
        "target_actor = MLP(state_size, action_size, num_hidden, hidden_size=512)\n",
        "target_critic = MLP(state_size + action_size, 1, num_hidden, hidden_size=512)\n",
        "initialize_weights(target_actor)\n",
        "initialize_weights(target_critic)\n",
        "\n",
        "# Training parameter\n",
        "gamma = 0.99\n",
        "max_epoche_number = 200\n",
        "batch_size = 128\n",
        "max_buffer_size = 50000\n",
        "max_action = 2.0  # Torque bounds in Pendulum-v1\n",
        "critic_learning_rate = 1e-3\n",
        "actor_learning_rate = 4e-4\n",
        "actor_loss = torch.tensor(0.0)\n",
        "\n",
        "optimiser_critic = optim.Adam(predictor_critic.parameters(), lr = critic_learning_rate)\n",
        "optimiser_actor = optim.Adam(predictor_actor.parameters(), lr = actor_learning_rate)\n",
        "\n",
        "state_list, action_list, reward_list, next_state_list = [], [], [], []\n",
        "total_reward_list, losses_critic, losses_actor = [], [], []\n",
        "\n",
        "epoch = 1\n",
        "previous_actor_loss = None  # Initialize previous actor loss\n",
        "\n",
        "while epoch <= max_epoche_number:\n",
        "\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "    state[2] /= 10.0  # Normalize theta_dot\n",
        "    done = False\n",
        "    experience_counter = 0\n",
        "    decay_factor = 0.998** epoch\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "        action = predictor_actor(state_tensor).detach().numpy()\n",
        "        noise_scale = max(0.8 * decay_factor, 0.05)\n",
        "        noisy_action = action + noise_scale * np.random.normal(0, 0.2, size=action.shape)\n",
        "        noisy_action = np.clip(noisy_action, -max_action, max_action)\n",
        "\n",
        "        next_state, reward, done, truncated = env.step(noisy_action)\n",
        "        done = done or truncated\n",
        "        next_state[2] /= 10.0  # Normalize theta_dot\n",
        "\n",
        "        # Store experience in the replay buffer\n",
        "        state_list.append(state)\n",
        "        action_list.append(noisy_action)\n",
        "        reward_list.append(reward)\n",
        "        next_state_list.append(next_state)\n",
        "        total_reward += reward\n",
        "        experience_counter += 1\n",
        "\n",
        "        # Trim buffer if necessary\n",
        "        if len(state_list) > max_buffer_size:\n",
        "            state_list = state_list[-max_buffer_size:]\n",
        "            action_list = action_list[-max_buffer_size:]\n",
        "            reward_list = reward_list[-max_buffer_size:]\n",
        "            next_state_list = next_state_list[-max_buffer_size:]\n",
        "\n",
        "        # Train if buffer has enough samples\n",
        "        if len(state_list) >= batch_size:\n",
        "            batched_state, batched_action, batched_reward, batched_next_state = batch_data(\n",
        "                state_list, action_list, reward_list, next_state_list, batch_size\n",
        "            )\n",
        "\n",
        "            target_miu = target_actor(batched_next_state)\n",
        "            target_q = target_critic(torch.cat([batched_next_state, target_miu], dim=1)).detach()\n",
        "            y = batched_reward + gamma * target_q\n",
        "\n",
        "            # Update critic\n",
        "            optimiser_critic.zero_grad()\n",
        "            critic_loss = MSE_loss(predictor_critic(torch.cat([batched_state, batched_action], dim=1)), y)\n",
        "            critic_loss.backward()\n",
        "            #torch.nn.utils.clip_grad_norm_(predictor_critic.parameters(), 1)  # Gradient clipping\n",
        "            optimiser_critic.step()\n",
        "\n",
        "            # Update actor\n",
        "            optimiser_actor.zero_grad()\n",
        "            actions_pred = predictor_actor(batched_state)\n",
        "            q_values = predictor_critic(torch.cat([batched_state, actions_pred], dim=1))\n",
        "            actor_loss = -q_values.mean()\n",
        "\n",
        "            for param_group in optimiser_actor.param_groups:\n",
        "                param_group['lr'] = -actor_learning_rate if actor_loss.item() < 0 else actor_learning_rate\n",
        "\n",
        "            actor_loss.backward()\n",
        "            #torch.nn.utils.clip_grad_norm_(predictor_actor.parameters(), 1.0)  # Gradient clipping\n",
        "            optimiser_actor.step()\n",
        "\n",
        "            # Update target networks\n",
        "            soft_update(target_actor, predictor_actor, tau = 0.005)\n",
        "            soft_update(target_critic, predictor_critic, tau = 0.005)\n",
        "\n",
        "            losses_critic.append(critic_loss.item())\n",
        "            losses_actor.append(actor_loss.item())\n",
        "\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    total_reward_list.append(total_reward)\n",
        "\n",
        "    # Update epoch\n",
        "    epoch += 1\n",
        "\n",
        "    # breaking the epoch loop after reaching the desired loss\n",
        "    if len(losses_actor) >= 20:\n",
        "        last_twenty_mean = sum(losses_actor[-20:]) / 20\n",
        "    else:\n",
        "       last_twenty_mean = sum(losses_actor[-20:]) / len(losses_actor)\n",
        "\n",
        "    #if last_twenty_mean < 0 and epoch > 10:\n",
        "     #   break\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch}, Total Reward: {sum(total_reward):.8f}, Actor Loss: {actor_loss.item():.16f}, Last Twenty Mean: {last_twenty_mean:.16f}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Plotting results\n",
        "smoothed_actor_loss = moving_average(losses_actor, window_size=100)\n",
        "plt.plot(smoothed_actor_loss)\n",
        "plt.title(\"Actor Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(moving_average(losses_critic, 10))\n",
        "plt.plot(moving_average(losses_actor, 10))\n",
        "plt.legend(['critic', 'actor'])\n",
        "plt.title(\"Losses\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(total_reward_list)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bk-PIDvy3-o"
      },
      "source": [
        "**Visualisation and Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sK7wN8a9GKC"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "cwd = os.getcwd()\n",
        "video_dir = os.path.join(cwd, 'learned_dynamics')\n",
        "if not os.path.isdir(video_dir):\n",
        "    os.mkdir(video_dir)\n",
        "video_file = os.path.join(video_dir, \"learned_dynamics.mp4\")\n",
        "\n",
        "# Create the Pendulum environment\n",
        "simulated_env = gym.make('Pendulum-v1', g=9.81, render_mode=\"rgb_array\")\n",
        "\n",
        "# Set up video recording\n",
        "video_recorder = VideoRecorder(simulated_env, video_file, enabled=True)\n",
        "\n",
        "# Reset the environment\n",
        "state = simulated_env.reset()\n",
        "if isinstance(state, tuple):  # Handle cases where reset returns a tuple\n",
        "    state = state[0]\n",
        "\n",
        "done = False\n",
        "\n",
        "# Perform an episode\n",
        "while not done:\n",
        "    video_recorder.capture_frame()\n",
        "\n",
        "    # Convert state to tensor\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "    # Predict action using the actor network\n",
        "    action = predictor_actor(state_tensor).detach().numpy()\n",
        "\n",
        "    # Ensure action is a single number (for Pendulum)\n",
        "    action = action.squeeze()  # Convert to scalar if it's an array of shape (1,)\n",
        "    action = np.clip(action, -max_action, max_action)\n",
        "\n",
        "    # Take a step in the environment\n",
        "    next_state, reward, done, truncated = simulated_env.step([action])  # Action must be a list for Pendulum\n",
        "    done = done or truncated\n",
        "    next_state[2] /= 10.0  # Normalize theta_dot\n",
        "    reward /= 1  # Normalize reward to [-1, 0]\n",
        "\n",
        "    # Transition to next state\n",
        "    state = next_state\n",
        "\n",
        "# Finalize video recording\n",
        "video_recorder.capture_frame()\n",
        "video_recorder.close()\n",
        "simulated_env.close()\n",
        "\n",
        "print(f\"Video saved in folder {video_dir}\")\n",
        "\n",
        "# Plot losses and rewards (assuming losses_critic, losses_actor, total_reward_list are defined)\n",
        "if len(losses_critic) > 0 and len(losses_actor) > 0:\n",
        "    plt.plot(moving_average(losses_critic, 10), label='Critic Loss')\n",
        "    plt.plot(moving_average(losses_actor, 10), label='Actor Loss')\n",
        "    plt.title(\"Losses for the Actor and the Critic Networks over 10 Moving Window\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Networks' Losses\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if len(total_reward_list) > 0:\n",
        "    plt.plot(total_reward_list)\n",
        "    plt.title(\"Total Rewards per Epoches\")\n",
        "    plt.xlabel(\"Epoches\")\n",
        "    plt.ylabel(\"Total Rewards\")\n",
        "    plt.show()\n",
        "\n",
        "if len(total_reward_list) > 0:\n",
        "    plt.plot(moving_average(total_reward_list, 10))\n",
        "    plt.title(\"Total Rewards per Epoches over 10 Moving Window\")\n",
        "    plt.xlabel(\"Epoches\")\n",
        "    plt.ylabel(\"Total Rewards\")\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GesRaGOmsZDB"
      },
      "source": [
        "**Phase Diagram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxIbnoqKsR4d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function to keep angles within the range [0, 360) degrees\n",
        "def normalize_angle_to_360(theta):\n",
        "    theta = np.rad2deg(theta)  # Convert to degrees\n",
        "    while theta < 0:\n",
        "        theta += 360\n",
        "    while theta >= 360:\n",
        "        theta -= 360\n",
        "    return theta\n",
        "\n",
        "# Number of random initial conditions to visualize\n",
        "num_trajectories = 5\n",
        "max_time_steps = 200  # Time limit for each trajectory\n",
        "\n",
        "# Store the data for plotting\n",
        "all_thetas = []\n",
        "all_theta_dots = []\n",
        "starting_points = []\n",
        "final_points = []\n",
        "\n",
        "for i in range(num_trajectories):\n",
        "    # Reset the environment and get the initial state\n",
        "    state = simulated_env.reset()\n",
        "    thetas = []\n",
        "    theta_dots = []\n",
        "\n",
        "    # Extract initial theta and theta_dot for bold marker\n",
        "    initial_theta = np.arctan2(state[1], state[0])  # Calculate the angle from cos(theta), sin(theta)\n",
        "    initial_theta_dot = state[2]\n",
        "    starting_points.append((normalize_angle_to_360(initial_theta), initial_theta_dot))\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        theta = np.arctan2(state[1], state[0])  # Calculate the angle from cos(theta), sin(theta)\n",
        "        theta = normalize_angle_to_360(theta)  # Normalize angle to [0, 360)\n",
        "\n",
        "        theta_dot = state[2]\n",
        "\n",
        "        thetas.append(theta)\n",
        "        theta_dots.append(theta_dot)\n",
        "\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "        # Predict action using the actor network\n",
        "        action = predictor_actor(state_tensor).detach().numpy()\n",
        "        action = np.clip(action.squeeze(), -max_action, max_action)\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_state, reward, done, truncated = simulated_env.step([action])\n",
        "        done = done or truncated\n",
        "        next_state[2] /= 10.0  # Normalize theta_dot\n",
        "        reward /= 1\n",
        "\n",
        "        # Transition to next state\n",
        "        state = next_state\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    all_thetas.append(thetas)\n",
        "    all_theta_dots.append(theta_dots)\n",
        "    final_points.append((thetas[-1], theta_dots[-1]))\n",
        "\n",
        "# Plotting the phase diagram\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot each trajectory with arrows\n",
        "for i in range(num_trajectories):\n",
        "    plt.plot(all_thetas[i], all_theta_dots[i], alpha=0.7, linewidth=1.5, label=f'Trajectory {i+1}' if i < 3 else '')\n",
        "\n",
        "# Highlight starting points with red dots\n",
        "starting_thetas, starting_theta_dots = zip(*starting_points)\n",
        "plt.scatter(starting_thetas, starting_theta_dots, color='red', s=50, label='Starting Points', edgecolors='black', linewidth=1.5)\n",
        "\n",
        "# Highlight final points with green dots\n",
        "final_thetas, final_theta_dots = zip(*final_points)\n",
        "plt.scatter(final_thetas, final_theta_dots, color='green', s=50, label='Final Points', edgecolors='black', linewidth=1.5)\n",
        "\n",
        "plt.xlabel(r'$\\theta$ (degrees)', fontsize=14)\n",
        "plt.ylabel(r'$\\dot{\\theta}$ (radians/s)', fontsize=14)\n",
        "plt.title('Phase Diagram of Pendulum - angular velocities (rad/s) vs angles (degrees)', fontsize=16)\n",
        "plt.axhline(0, color='black', linestyle='--', linewidth=0.8)  # Horizontal line at y=0\n",
        "plt.axvline(0, color='black', linestyle='--', linewidth=0.8)  # Vertical line at x=0\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0HuBn6_sIjV"
      },
      "source": [
        "**Histogram of the Final Angles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBwty4pMonTG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Number of random initial conditions to visualize\n",
        "num_trajectories = 100  # Number of trajectories to simulate\n",
        "max_time_steps = 200  # Time limit for each trajectory\n",
        "\n",
        "# Store final angles\n",
        "final_angles = []\n",
        "\n",
        "for i in range(num_trajectories):\n",
        "    # Reset the environment and get the initial state\n",
        "    state = simulated_env.reset()\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        theta = np.arctan2(state[1], state[0])  # Calculate the angle from cos(theta), sin(theta)\n",
        "        theta = np.rad2deg(theta)  # Convert to degrees\n",
        "\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "        # Predict action using the actor network\n",
        "        action = predictor_actor(state_tensor).detach().numpy()\n",
        "        action = np.clip(action.squeeze(), -max_action, max_action)\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_state, reward, done, truncated = simulated_env.step([action])\n",
        "        done = done or truncated\n",
        "        next_state[2] /= 10.0  # Normalize theta_dot\n",
        "        reward /= 1\n",
        "\n",
        "        # Transition to next state\n",
        "        state = next_state\n",
        "\n",
        "        if done or truncated:\n",
        "            break  # Stop simulation when it ends\n",
        "\n",
        "    # Append final angle to the list (constrained between -180 and 180)\n",
        "    final_angles.append(theta)\n",
        "\n",
        "# Ensure final_angles contains only the last angle of each trajectory\n",
        "# Plotting the histogram of final angles\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(final_angles, bins=60, range=(-180, 180), color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Upright Position (0)')\n",
        "plt.xlabel('Final Angle (Degrees)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Histogram of Final Angles', fontsize = 12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.xticks(np.arange(-180, 181, 45))  # Set x-axis ticks for better readability\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Final angles data\n",
        "final_angles = np.array(final_angles)\n",
        "\n",
        "# Perform one-sample t-test\n",
        "t_stat, p_value = ttest_1samp(final_angles, 0)\n",
        "\n",
        "# Print results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The mean is significantly different from 0.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The mean is not significantly different from 0.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}