{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alireza8Kh/the-DDPG-Method-Implemented-in-Pendulum-v1-Environment/blob/main/Copy_of_Pendulum_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G3elMwDhOkOz"
      },
      "outputs": [],
      "source": [
        "#!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "20IyxDzgp3tU"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "\n",
        "import os\n",
        "import gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Graphical library\n",
        "import random\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from scipy.stats import ttest_1samp\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Configuring Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Tox87FUXwD"
      },
      "source": [
        "**Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GmBYBmriPSZ",
        "outputId": "3be37ab0-e730-40fc-8493-36c81791ac66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment PendulumPrediction-v1\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Pendulum-v1', g = 9.81)\n",
        "\n",
        "from gym.wrappers.time_limit import TimeLimit\n",
        "\n",
        "\n",
        "class ShowPendulumPredictions(gym.Env):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.env = gym.make('Pendulum-v1', g=9.81)\n",
        "        self.action_space = self.env.action_space\n",
        "        self.observation_space = self.env.observation_space\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "\n",
        "        return self.env.render(mode=mode)\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Close the environment.\n",
        "        \"\"\"\n",
        "        self.env.close()\n",
        "\n",
        "gym.envs.registration.register( id=\"PendulumPrediction-v1\", entry_point = ShowPendulumPredictions, max_episode_steps=999 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKHIOPm_UwK4"
      },
      "source": [
        "**the MLP class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZrrOYjZfcEIA"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model):\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, nn.Linear):  # Check if the layer is a linear layer\n",
        "            nn.init.xavier_uniform_(layer.weight)  # Xavier initialization for weights\n",
        "            if layer.bias is not None:  # Initialize biases to zeros\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, num_hidden, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialise the network.\n",
        "        Input:\n",
        "            - input_size {int} - size of input to the network\n",
        "            - output_size {int} - size of output to the network\n",
        "            - num_hidden {int} - number of hidden layers\n",
        "            - hidden_size {int} - size of each hidden layer\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size) # First tranformation from the network input to the input of first hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden-1)]) # All the hidden transformation\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size) # Last tranformation from the last hidden layer output to the network output\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Get the output of the MLP.\n",
        "        Input: x {tensor} - one element or a batch of element\n",
        "        Ouput: y {tensor} - corresponding output\n",
        "        \"\"\"\n",
        "        x.to(device)\n",
        "        x = self.input_layer(x) # Passing through the input layer\n",
        "        x = F.relu(x) # Applying Relu activation\n",
        "        for layer in self.hidden_layers:\n",
        "          x = layer(x) # Passing through each hidden layer\n",
        "          x = F.relu(x) # Applying Relu activation\n",
        "        x = self.output_layer(x) # Passing through the output layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lBcIUZ1x6Bg"
      },
      "source": [
        "**Batching the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aydDmDSz3olA"
      },
      "outputs": [],
      "source": [
        "def batch_data(state_list, action_list, reward_list, next_state_list, batch_size):\n",
        "    num_samples = len(state_list)\n",
        "    sample_indices = random.sample(range(num_samples), batch_size)\n",
        "\n",
        "\n",
        "    # Ensure tensors are correctly reshaped\n",
        "    batched_state = torch.cat(\n",
        "        [torch.tensor(state_list[i], dtype=torch.float32).view(1, -1) for i in sample_indices], dim=0\n",
        "    )\n",
        "    batched_action = torch.cat(\n",
        "        [torch.tensor(action_list[i], dtype=torch.float32).view(1, -1) for i in sample_indices], dim=0\n",
        "    )\n",
        "    batched_reward = torch.cat(\n",
        "        [torch.tensor([reward_list[i]], dtype=torch.float32).view(1, -1) for i in sample_indices], dim=0\n",
        "    )\n",
        "    batched_next_state = torch.cat(\n",
        "        [torch.tensor(next_state_list[i], dtype=torch.float32).view(1, -1) for i in sample_indices], dim=0\n",
        "    )\n",
        "\n",
        "    return batched_state, batched_action, batched_reward, batched_next_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI9jgtoRdE0B"
      },
      "source": [
        "**Some Usefull Modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Jek9Y9dvjLvr"
      },
      "outputs": [],
      "source": [
        "def MSE_loss(prediction, target):\n",
        "\n",
        "      return ((prediction - target)**2).sum(dim=-1).mean()\n",
        "\n",
        "def soft_update(target_network, predictor_network, tau):\n",
        "        \"\"\"\n",
        "        Softly updates the parameters of the target network.\n",
        "        target_param = tau * predictor_param + (1 - tau) * target_param\n",
        "        \"\"\"\n",
        "        for target_param, predictor_param in zip(target_network.parameters(), predictor_network.parameters()):\n",
        "            target_param.data.copy_(tau * predictor_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "def reset(self):\n",
        "        \"\"\"Reset the noise process to the mean.\"\"\"\n",
        "        self.x_prev = np.zeros_like(self.mu)\n",
        "\n",
        "def sample(self):\n",
        "        \"\"\"Generate the next noise value.\"\"\"\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mu - self.x_prev) * self.dt\n",
        "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.action_dim)\n",
        "        )\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "def soft_update(target, source, tau):\n",
        "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
        "            target_param.data.copy_(tau * source_param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "def moving_average(data, window_size=10):\n",
        "      return [sum(data[i:i+window_size])/window_size for i in range(len(data) - window_size)]"
      ]

      "source": [
        "state_size = 3  # [cos(theta), sin(theta), theta_dot]\n",
        "action_size = 1  # Torque\n",
        "num_hidden = 2\n",
        "\n",
        "# Define networks\n",
        "predictor_actor = MLP(state_size, action_size, num_hidden, hidden_size=512)\n",
        "predictor_critic = MLP(state_size + action_size, 1, num_hidden, hidden_size=512)\n",
        "initialize_weights(predictor_actor)\n",
        "initialize_weights(predictor_critic)\n",
        "\n",
        "target_actor = MLP(state_size, action_size, num_hidden, hidden_size=512)\n",
        "target_critic = MLP(state_size + action_size, 1, num_hidden, hidden_size=512)\n",
        "initialize_weights(target_actor)\n",
        "initialize_weights(target_critic)\n",
        "\n",
        "# Training parameter\n",
        "gamma = 0.99\n",
        "max_epoche_number = 200\n",
        "batch_size = 128\n",
        "max_buffer_size = 50000\n",
        "max_action = 2.0  # Torque bounds in Pendulum-v1\n",
        "critic_learning_rate = 1e-3\n",
        "actor_learning_rate = 4e-4\n",
        "actor_loss = torch.tensor(0.0)\n",
        "\n",
        "optimiser_critic = optim.Adam(predictor_critic.parameters(), lr = critic_learning_rate)\n",
        "optimiser_actor = optim.Adam(predictor_actor.parameters(), lr = actor_learning_rate)\n",
        "\n",
        "state_list, action_list, reward_list, next_state_list = [], [], [], []\n",
        "total_reward_list, losses_critic, losses_actor = [], [], []\n",
        "\n",
        "epoch = 1\n",
        "previous_actor_loss = None  # Initialize previous actor loss\n",
        "\n",
        "while epoch <= max_epoche_number:\n",
        "\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "    state[2] /= 10.0  # Normalize theta_dot\n",
        "    done = False\n",
        "    experience_counter = 0\n",
        "    decay_factor = 0.998** epoch\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "        action = predictor_actor(state_tensor).detach().numpy()\n",
        "        noise_scale = max(0.8 * decay_factor, 0.05)\n",
        "        noisy_action = action + noise_scale * np.random.normal(0, 0.2, size=action.shape)\n",
        "        noisy_action = np.clip(noisy_action, -max_action, max_action)\n",
        "\n",
        "        next_state, reward, done, truncated = env.step(noisy_action)\n",
        "        done = done or truncated\n",
        "        next_state[2] /= 10.0  # Normalize theta_dot\n",
        "\n",
        "        # Store experience in the replay buffer\n",
        "        state_list.append(state)\n",
        "        action_list.append(noisy_action)\n",
        "        reward_list.append(reward)\n",
        "        next_state_list.append(next_state)\n",
        "        total_reward += reward\n",
        "        experience_counter += 1\n",
        "\n",
        "        # Trim buffer if necessary\n",
        "        if len(state_list) > max_buffer_size:\n",
        "            state_list = state_list[-max_buffer_size:]\n",
        "            action_list = action_list[-max_buffer_size:]\n",
        "            reward_list = reward_list[-max_buffer_size:]\n",
        "            next_state_list = next_state_list[-max_buffer_size:]\n",
        "\n",
        "        # Train if buffer has enough samples\n",
        "        if len(state_list) >= batch_size:\n",
        "            batched_state, batched_action, batched_reward, batched_next_state = batch_data(\n",
        "                state_list, action_list, reward_list, next_state_list, batch_size\n",
        "            )\n",
        "\n",
        "            target_miu = target_actor(batched_next_state)\n",
        "            target_q = target_critic(torch.cat([batched_next_state, target_miu], dim=1)).detach()\n",
        "            y = batched_reward + gamma * target_q\n",
        "\n",
        "            # Update critic\n",
        "            optimiser_critic.zero_grad()\n",
        "            critic_loss = MSE_loss(predictor_critic(torch.cat([batched_state, batched_action], dim=1)), y)\n",
        "            critic_loss.backward()\n",
        "            #torch.nn.utils.clip_grad_norm_(predictor_critic.parameters(), 1)  # Gradient clipping\n",
        "            optimiser_critic.step()\n",
        "\n",
        "            # Update actor\n",
        "            optimiser_actor.zero_grad()\n",
        "            actions_pred = predictor_actor(batched_state)\n",
        "            q_values = predictor_critic(torch.cat([batched_state, actions_pred], dim=1))\n",
        "            actor_loss = -q_values.mean()\n",
        "\n",
        "            for param_group in optimiser_actor.param_groups:\n",
        "                param_group['lr'] = -actor_learning_rate if actor_loss.item() < 0 else actor_learning_rate\n",
        "\n",
        "            actor_loss.backward()\n",
        "            #torch.nn.utils.clip_grad_norm_(predictor_actor.parameters(), 1.0)  # Gradient clipping\n",
        "            optimiser_actor.step()\n",
        "\n",
        "            # Update target networks\n",
        "            soft_update(target_actor, predictor_actor, tau = 0.005)\n",
        "            soft_update(target_critic, predictor_critic, tau = 0.005)\n",
        "\n",
        "            losses_critic.append(critic_loss.item())\n",
        "            losses_actor.append(actor_loss.item())\n",
        "\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    total_reward_list.append(total_reward)\n",
        "\n",
        "    # Update epoch\n",
        "    epoch += 1\n",
        "\n",
        "    # breaking the epoch loop after reaching the desired loss\n",
        "    if len(losses_actor) >= 20:\n",
        "        last_twenty_mean = sum(losses_actor[-20:]) / 20\n",
        "    else:\n",
        "       last_twenty_mean = sum(losses_actor[-20:]) / len(losses_actor)\n",
        "\n",
        "    #if last_twenty_mean < 0 and epoch > 10:\n",
        "     #   break\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch}, Total Reward: {sum(total_reward):.8f}, Actor Loss: {actor_loss.item():.16f}, Last Twenty Mean: {last_twenty_mean:.16f}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Plotting results\n",
        "smoothed_actor_loss = moving_average(losses_actor, window_size=100)\n",
        "plt.plot(smoothed_actor_loss)\n",
        "plt.title(\"Actor Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(moving_average(losses_critic, 10))\n",
        "plt.plot(moving_average(losses_actor, 10))\n",
        "plt.legend(['critic', 'actor'])\n",
        "plt.title(\"Losses\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(total_reward_list)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bk-PIDvy3-o"
      },
      "source": [
        "**Visualisation and Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sK7wN8a9GKC"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "cwd = os.getcwd()\n",
        "video_dir = os.path.join(cwd, 'learned_dynamics')\n",
        "if not os.path.isdir(video_dir):\n",
        "    os.mkdir(video_dir)\n",
        "video_file = os.path.join(video_dir, \"learned_dynamics.mp4\")\n",
        "\n",
        "# Create the Pendulum environment\n",
        "simulated_env = gym.make('Pendulum-v1', g=9.81, render_mode=\"rgb_array\")\n",
        "\n",
        "# Set up video recording\n",
        "video_recorder = VideoRecorder(simulated_env, video_file, enabled=True)\n",
        "\n",
        "# Reset the environment\n",
        "state = simulated_env.reset()\n",
        "if isinstance(state, tuple):  # Handle cases where reset returns a tuple\n",
        "    state = state[0]\n",
        "\n",
        "done = False\n",
        "\n",
        "# Perform an episode\n",
        "while not done:\n",
        "    video_recorder.capture_frame()\n",
        "\n",
        "    # Convert state to tensor\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "    # Predict action using the actor network\n",
        "    action = predictor_actor(state_tensor).detach().numpy()\n",
        "\n",
        "    # Ensure action is a single number (for Pendulum)\n",
        "    action = action.squeeze()  # Convert to scalar if it's an array of shape (1,)\n",
        "    action = np.clip(action, -max_action, max_action)\n",
        "\n",
        "    # Take a step in the environment\n",
        "    next_state, reward, done, truncated = simulated_env.step([action])  # Action must be a list for Pendulum\n",
        "    done = done or truncated\n",
        "    next_state[2] /= 10.0  # Normalize theta_dot\n",
        "    reward /= 1  # Normalize reward to [-1, 0]\n",
        "\n",
        "    # Transition to next state\n",
        "    state = next_state\n",
        "\n",
        "# Finalize video recording\n",
        "video_recorder.capture_frame()\n",
        "video_recorder.close()\n",
        "simulated_env.close()\n",
        "\n",
        "print(f\"Video saved in folder {video_dir}\")\n",
        "\n",
        "# Plot losses and rewards (assuming losses_critic, losses_actor, total_reward_list are defined)\n",
        "if len(losses_critic) > 0 and len(losses_actor) > 0:\n",
        "    plt.plot(moving_average(losses_critic, 10), label='Critic Loss')\n",
        "    plt.plot(moving_average(losses_actor, 10), label='Actor Loss')\n",
        "    plt.title(\"Losses for the Actor and the Critic Networks over 10 Moving Window\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Networks' Losses\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if len(total_reward_list) > 0:\n",
        "    plt.plot(total_reward_list)\n",
        "    plt.title(\"Total Rewards per Epoches\")\n",
        "    plt.xlabel(\"Epoches\")\n",
        "    plt.ylabel(\"Total Rewards\")\n",
        "    plt.show()\n",
        "\n",
        "if len(total_reward_list) > 0:\n",
        "    plt.plot(moving_average(total_reward_list, 10))\n",
        "    plt.title(\"Total Rewards per Epoches over 10 Moving Window\")\n",
        "    plt.xlabel(\"Epoches\")\n",
        "    plt.ylabel(\"Total Rewards\")\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GesRaGOmsZDB"
      },
      "source": [
        "**Phase Diagram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxIbnoqKsR4d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function to keep angles within the range [0, 360) degrees\n",
        "def normalize_angle_to_360(theta):\n",
        "    theta = np.rad2deg(theta)  # Convert to degrees\n",
        "    while theta < 0:\n",
        "        theta += 360\n",
        "    while theta >= 360:\n",
        "        theta -= 360\n",
        "    return theta\n",
        "\n",
        "# Number of random initial conditions to visualize\n",
        "num_trajectories = 5\n",
        "max_time_steps = 200  # Time limit for each trajectory\n",
        "\n",
        "# Store the data for plotting\n",
        "all_thetas = []\n",
        "all_theta_dots = []\n",
        "starting_points = []\n",
        "final_points = []\n",
        "\n",
        "for i in range(num_trajectories):\n",
        "    # Reset the environment and get the initial state\n",
        "    state = simulated_env.reset()\n",
        "    thetas = []\n",
        "    theta_dots = []\n",
        "\n",
        "    # Extract initial theta and theta_dot for bold marker\n",
        "    initial_theta = np.arctan2(state[1], state[0])  # Calculate the angle from cos(theta), sin(theta)\n",
        "    initial_theta_dot = state[2]\n",
        "    starting_points.append((normalize_angle_to_360(initial_theta), initial_theta_dot))\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        theta = np.arctan2(state[1], state[0])  # Calculate the angle from cos(theta), sin(theta)\n",
        "        theta = normalize_angle_to_360(theta)  # Normalize angle to [0, 360)\n",
        "\n",
        "        theta_dot = state[2]\n",
        "\n",
        "        thetas.append(theta)\n",
        "        theta_dots.append(theta_dot)\n",
        "\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "        # Predict action using the actor network\n",
        "        action = predictor_actor(state_tensor).detach().numpy()\n",
        "        action = np.clip(action.squeeze(), -max_action, max_action)\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_state, reward, done, truncated = simulated_env.step([action])\n",
        "        done = done or truncated\n",
        "        next_state[2] /= 10.0  # Normalize theta_dot\n",
        "        reward /= 1\n",
        "\n",
        "        # Transition to next state\n",
        "        state = next_state\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    all_thetas.append(thetas)\n",
        "    all_theta_dots.append(theta_dots)\n",
        "    final_points.append((thetas[-1], theta_dots[-1]))\n",
        "\n",
        "# Plotting the phase diagram\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot each trajectory with arrows\n",
        "for i in range(num_trajectories):\n",
        "    plt.plot(all_thetas[i], all_theta_dots[i], alpha=0.7, linewidth=1.5, label=f'Trajectory {i+1}' if i < 3 else '')\n",
        "\n",
        "# Highlight starting points with red dots\n",
        "starting_thetas, starting_theta_dots = zip(*starting_points)\n",
        "plt.scatter(starting_thetas, starting_theta_dots, color='red', s=50, label='Starting Points', edgecolors='black', linewidth=1.5)\n",
        "\n",
        "# Highlight final points with green dots\n",
        "final_thetas, final_theta_dots = zip(*final_points)\n",
        "plt.scatter(final_thetas, final_theta_dots, color='green', s=50, label='Final Points', edgecolors='black', linewidth=1.5)\n",
        "\n",
        "plt.xlabel(r'$\\theta$ (degrees)', fontsize=14)\n",
        "plt.ylabel(r'$\\dot{\\theta}$ (radians/s)', fontsize=14)\n",
        "plt.title('Phase Diagram of Pendulum - angular velocities (rad/s) vs angles (degrees)', fontsize=16)\n",
        "plt.axhline(0, color='black', linestyle='--', linewidth=0.8)  # Horizontal line at y=0\n",
        "plt.axvline(0, color='black', linestyle='--', linewidth=0.8)  # Vertical line at x=0\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0HuBn6_sIjV"
      },
      "source": [
        "**Histogram of the Final Angles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBwty4pMonTG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Number of random initial conditions to visualize\n",
        "num_trajectories = 100  # Number of trajectories to simulate\n",
        "max_time_steps = 200  # Time limit for each trajectory\n",
        "\n",
        "# Store final angles\n",
        "final_angles = []\n",
        "\n",
        "for i in range(num_trajectories):\n",
        "    # Reset the environment and get the initial state\n",
        "    state = simulated_env.reset()\n",
        "\n",
        "    for t in range(max_time_steps):\n",
        "        theta = np.arctan2(state[1], state[0])  # Calculate the angle from cos(theta), sin(theta)\n",
        "        theta = np.rad2deg(theta)  # Convert to degrees\n",
        "\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "        # Predict action using the actor network\n",
        "        action = predictor_actor(state_tensor).detach().numpy()\n",
        "        action = np.clip(action.squeeze(), -max_action, max_action)\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_state, reward, done, truncated = simulated_env.step([action])\n",
        "        done = done or truncated\n",
        "        next_state[2] /= 10.0  # Normalize theta_dot\n",
        "        reward /= 1\n",
        "\n",
        "        # Transition to next state\n",
        "        state = next_state\n",
        "\n",
        "        if done or truncated:\n",
        "            break  # Stop simulation when it ends\n",
        "\n",
        "    # Append final angle to the list (constrained between -180 and 180)\n",
        "    final_angles.append(theta)\n",
        "\n",
        "# Ensure final_angles contains only the last angle of each trajectory\n",
        "# Plotting the histogram of final angles\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(final_angles, bins=60, range=(-180, 180), color='blue', alpha=0.7, edgecolor='black')\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Upright Position (0°)')\n",
        "plt.xlabel('Final Angle (Degrees)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Histogram of Final Angles', fontsize = 12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.xticks(np.arange(-180, 181, 45))  # Set x-axis ticks for better readability\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Final angles data\n",
        "final_angles = np.array(final_angles)\n",
        "\n",
        "# Perform one-sample t-test\n",
        "t_stat, p_value = ttest_1samp(final_angles, 0)\n",
        "\n",
        "# Print results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The mean is significantly different from 0°.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The mean is not significantly different from 0°.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
